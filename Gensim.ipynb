{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPNSBMQqpwL6DlTIogi+B6h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cunUjCGtRrIk","executionInfo":{"status":"ok","timestamp":1719249909646,"user_tz":-330,"elapsed":20311,"user":{"displayName":"Ambika M K","userId":"12867637850770584710"}},"outputId":"37e74b4b-c7c6-4c25-876c-54ca01538df9"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Enter the text to be processed: My Name is Ambika\n","\n","Tokens: ['my', 'name', 'is', 'ambika']\n","\n","Lemmatized Tokens: ['my', 'name', 'is', 'ambika']\n","\n","Stemmed Tokens: ['my', 'name', 'is', 'ambika']\n","\n","Filtered Tokens (Stop Words Removed): ['ambika']\n"]}],"source":["import gensim\n","from gensim.utils import simple_preprocess\n","from gensim.parsing.preprocessing import STOPWORDS\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","\n","# Download NLTK resources if not already downloaded\n","import nltk\n","nltk.download('wordnet')\n","\n","# Take input from the user\n","text = input(\"Enter the text to be processed: \")\n","\n","# Tokenization using Gensim\n","tokens = simple_preprocess(text, deacc=True)\n","print(f\"\\nTokens: {tokens}\")\n","\n","# Lemmatization using NLTK's WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","print(f\"\\nLemmatized Tokens: {lemmatized_tokens}\")\n","\n","# Stemming using NLTK's PorterStemmer\n","stemmer = PorterStemmer()\n","stemmed_tokens = [stemmer.stem(token) for token in tokens]\n","print(f\"\\nStemmed Tokens: {stemmed_tokens}\")\n","\n","# Stop word removal using Gensim's built-in stop words\n","stop_words = STOPWORDS\n","filtered_tokens = [token for token in tokens if token not in stop_words]\n","print(f\"\\nFiltered Tokens (Stop Words Removed): {filtered_tokens}\")\n"]}]}